{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.16.2)\n",
      "Requirement already satisfied: filelock in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: requests in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/cz/d_k7_tfn267938ylcg0bywxr0000gn/T/pip-req-build-19eug23k\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/cz/d_k7_tfn267938ylcg0bywxr0000gn/T/pip-req-build-19eug23k\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from clip==1.0) (2.1.2)\n",
      "Requirement already satisfied: torchvision in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from clip==1.0) (0.16.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch->clip==1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch->clip==1.0) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.26.0)\n",
      "Requirement already satisfied: requests in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision->clip==1.0) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.11.3)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scipy) (1.26.0)\n",
      "Requirement already satisfied: gensim in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from gensim) (7.0.3)\n",
      "Requirement already satisfied: wrapt in /Users/rishabhthaney/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install scipy\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image):\n",
    "    preprocessed_image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(preprocessed_image)\n",
    "        return image_features\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    with torch.no_grad():\n",
    "        return model.encode_text(clip.tokenize([text]).to(device))\n",
    "\n",
    "def get_text_embedding_word2vec(text):\n",
    "    words = text.split()\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in w2v_model.key_to_index:\n",
    "            word_embedding = w2v_model.get_vector(word)\n",
    "            embeddings.append(word_embedding)\n",
    "\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    text_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "    # Convert the NumPy array to a torch tensor\n",
    "    text_embedding_tensor = torch.tensor(text_embedding)\n",
    "\n",
    "    return text_embedding_tensor\n",
    "\n",
    "def get_images_from_folder(folder_path):\n",
    "    images = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is an image file\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "            # Open the image using PIL\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            # Append the image to the list\n",
    "            images.append(get_image_embedding(image))\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def find_image_embedding_arithmetics(pair1,pair2):\n",
    "    first_pair = pair1.split(':')\n",
    "    second_pair = pair2.split(':')\n",
    "\n",
    "    first_image_embeddings = get_images_from_folder(os.path.join(\"new_images\" ,first_pair[0]))\n",
    "    second_image_embeddings = get_images_from_folder(os.path.join(\"new_images\" ,first_pair[1]))\n",
    "    third_image_embeddings = get_images_from_folder(os.path.join(\"new_images\" ,second_pair[0]))\n",
    "    fourth_image_embeddings = get_images_from_folder(os.path.join(\"new_images\" ,second_pair[1]))\n",
    "\n",
    "\n",
    "    first_image_embeddings = torch.squeeze(torch.stack(first_image_embeddings, dim=0), dim=1)\n",
    "    second_image_embeddings = torch.squeeze(torch.stack(second_image_embeddings, dim=0), dim=1)\n",
    "    third_image_embeddings = torch.squeeze(torch.stack(third_image_embeddings, dim=0), dim=1)\n",
    "    fourth_image_embeddings = torch.squeeze(torch.stack(fourth_image_embeddings, dim=0), dim=1)\n",
    "\n",
    "    # Average of embeddings\n",
    "    first_image_embeddings = torch.mean(first_image_embeddings, dim=0)\n",
    "    second_image_embeddings = torch.mean(second_image_embeddings, dim=0)\n",
    "    third_image_embeddings = torch.mean(third_image_embeddings, dim=0)\n",
    "    fourth_image_embeddings = torch.mean(fourth_image_embeddings, dim=0)\n",
    "\n",
    "\n",
    "    offset_vector_embedding = second_image_embeddings - first_image_embeddings + third_image_embeddings\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth_image_embeddings, dim=0)\n",
    "\n",
    "    cos_image_before_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "\n",
    "    print(\"Angle for Image embedding before normalizing: Averaging \", cos_image_before_normalization)\n",
    "\n",
    "    rho_image_before_normalization, p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth_image_embeddings.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for Image embedding before normalizing:\", rho_image_before_normalization)\n",
    "\n",
    "\n",
    "    first_image_embeddings = F.normalize(first_image_embeddings , p=2,dim=0)\n",
    "    second_image_embeddings = F.normalize(second_image_embeddings , p=2,dim=0)\n",
    "    third_image_embeddings = F.normalize(third_image_embeddings, p=2,dim=0)\n",
    "    fourth_image_embeddings = F.normalize(fourth_image_embeddings , p=2,dim=0)\n",
    "\n",
    "    offset_vector_embedding = second_image_embeddings - first_image_embeddings + third_image_embeddings\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth_image_embeddings, dim=0)\n",
    "\n",
    "    cos_image_after_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "\n",
    "    print(\"Angle for Image embedding after normalizing: Averaging \", cos_image_after_normalization)\n",
    "\n",
    "    rho_image_after_normalization, p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth_image_embeddings.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for Image embedding after normalizing:\", rho_image_after_normalization)\n",
    "\n",
    "    return cos_image_before_normalization, rho_image_before_normalization,cos_image_after_normalization, rho_image_after_normalization, offset_vector_embedding, fourth_image_embeddings\n",
    "\n",
    "\n",
    "def find_text_embedding_arithmetics(pair1,pair2): \n",
    "    first_pair = pair1.split(':')\n",
    "    second_pair = pair2.split(':')\n",
    "\n",
    "    first = get_text_embedding(first_pair[0]).squeeze(0)\n",
    "    second = get_text_embedding(first_pair[1]).squeeze(0)\n",
    "    third = get_text_embedding(second_pair[0]).squeeze(0)\n",
    "    fourth = get_text_embedding(second_pair[1]).squeeze(0)\n",
    "\n",
    "    offset_vector_embedding = second - first + third\n",
    "\n",
    "    # King - Queen + Man = Woman\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth, dim=0)\n",
    "\n",
    "    cos_text_before_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "    \n",
    "\n",
    "    print(\"Angle for text embedding before normalizing: Averaging \", cos_text_before_normalization)\n",
    "    rho_text_before_normalization , p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for text embedding before normalizing:\", rho_text_before_normalization)\n",
    "\n",
    "\n",
    "    first = F.normalize(first , p=2,dim=0)\n",
    "    second = F.normalize(second , p=2,dim=0)\n",
    "    third = F.normalize(third , p=2,dim=0)\n",
    "    fourth = F.normalize(fourth, p=2,dim=0)\n",
    "\n",
    "    offset_vector_embedding = second - first + third\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth, dim=0)\n",
    "\n",
    "    cos_text_after_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "\n",
    "    print(\"Angle for text embedding after normalizing: Averaging \", cos_text_after_normalization)\n",
    "    rho_text_after_normalization, p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for text embedding after normalizing:\", rho_text_after_normalization)\n",
    "\n",
    "    return cos_text_before_normalization, rho_text_before_normalization, cos_text_after_normalization, rho_text_after_normalization, offset_vector_embedding, fourth\n",
    "\n",
    "def find_w2v_text_embedding_arithmetics(pair1,pair2): \n",
    "    first_pair = pair1.split(':')\n",
    "    second_pair = pair2.split(':')\n",
    "\n",
    "    w2v_first = get_text_embedding_word2vec(first_pair[0])\n",
    "    w2v_second = get_text_embedding_word2vec(first_pair[1])\n",
    "    w2v_third = get_text_embedding_word2vec(second_pair[0])\n",
    "    w2v_fourth = get_text_embedding_word2vec(second_pair[1])\n",
    "\n",
    "    w2v_offset_vector_embedding = w2v_second - w2v_first + w2v_third\n",
    "\n",
    "    # King - Queen + Man = Woman\n",
    "\n",
    "    w2v_cos_sim = F.cosine_similarity(w2v_offset_vector_embedding, w2v_fourth, dim=0)\n",
    "\n",
    "    w2v_cos_text_before_normalization = torch.rad2deg(torch.acos(w2v_cos_sim))\n",
    "    \n",
    "\n",
    "    print(\"Word2Vec Angle for text embedding before normalizing: Averaging \", w2v_cos_text_before_normalization)\n",
    "    w2v_rho_text_before_normalization , w2v_p_value = spearmanr(w2v_offset_vector_embedding.cpu().numpy(), w2v_fourth.cpu().numpy())\n",
    "    print(\"Word2Vec Spearman's correlation coefficient for text embedding before normalizing:\", w2v_rho_text_before_normalization)\n",
    "\n",
    "\n",
    "    w2v_first = F.normalize(w2v_first , p=2,dim=0)\n",
    "    w2v_second = F.normalize(w2v_second , p=2,dim=0)\n",
    "    w2v_third = F.normalize(w2v_third , p=2,dim=0)\n",
    "    w2v_fourth = F.normalize(w2v_fourth, p=2,dim=0)\n",
    "\n",
    "    w2v_offset_vector_embedding = w2v_second - w2v_first + w2v_third\n",
    "\n",
    "    w2v_cos_sim = F.cosine_similarity(w2v_offset_vector_embedding, w2v_fourth, dim=0)\n",
    "\n",
    "    w2v_cos_text_after_normalization = torch.rad2deg(torch.acos(w2v_cos_sim))\n",
    "\n",
    "    print(\"Word2Vec Angle for text embedding after normalizing: Averaging \", w2v_cos_text_after_normalization)\n",
    "    w2v_rho_text_after_normalization, w2v_p_value = spearmanr(w2v_offset_vector_embedding.cpu().numpy(), w2v_fourth.cpu().numpy())\n",
    "    print(\"Word2Vec Spearman's correlation coefficient for text embedding after normalizing:\", w2v_rho_text_after_normalization)\n",
    "\n",
    "    return w2v_cos_text_before_normalization, w2v_rho_text_before_normalization, w2v_cos_text_after_normalization, w2v_rho_text_after_normalization, w2v_offset_vector_embedding, w2v_fourth\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cosine_similarity(): argument 'x1' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pair1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhour:seconds\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m pair2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfeet:inches\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m find_w2v_text_embedding_arithmetics(pair1,pair2)\n",
      "Cell \u001b[0;32mIn[29], line 109\u001b[0m, in \u001b[0;36mfind_w2v_text_embedding_arithmetics\u001b[0;34m(pair1, pair2)\u001b[0m\n\u001b[1;32m    105\u001b[0m w2v_offset_vector_embedding \u001b[39m=\u001b[39m w2v_second \u001b[39m-\u001b[39m w2v_first \u001b[39m+\u001b[39m w2v_third\n\u001b[1;32m    107\u001b[0m \u001b[39m# King - Queen + Man = Woman\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m w2v_cos_sim \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcosine_similarity(w2v_offset_vector_embedding, w2v_fourth, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    111\u001b[0m w2v_cos_text_before_normalization \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrad2deg(torch\u001b[39m.\u001b[39macos(w2v_cos_sim))\n\u001b[1;32m    114\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWord2Vec Angle for text embedding before normalizing: Averaging \u001b[39m\u001b[39m\"\u001b[39m, w2v_cos_text_before_normalization)\n",
      "\u001b[0;31mTypeError\u001b[0m: cosine_similarity(): argument 'x1' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "pair1 = \"hour:seconds\"\n",
    "pair2 = \"feet:inches\"\n",
    "find_w2v_text_embedding_arithmetics(pair1,pair2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
