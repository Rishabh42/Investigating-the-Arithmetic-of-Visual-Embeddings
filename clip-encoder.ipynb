{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: filelock in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\shwet\\appdata\\local\\temp\\pip-req-build-19v4lqzf\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from clip==1.0) (6.1.3)\n",
      "Requirement already satisfied: regex in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from clip==1.0) (2.2.1)\n",
      "Requirement already satisfied: torchvision in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from clip==1.0) (0.17.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch->clip==1.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: numpy in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torchvision->clip==1.0) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from torchvision->clip==1.0) (10.2.0)\n",
      "Requirement already satisfied: colorama in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\shwet\\AppData\\Local\\Temp\\pip-req-build-19v4lqzf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in d:\\ms\\comp545\\project\\investigating the arithmetic of visual embeddings\\.venv\\lib\\site-packages (from scipy) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image):\n",
    "    preprocessed_image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(preprocessed_image)\n",
    "        return image_features\n",
    "    \n",
    "def get_text_embedding(text):\n",
    "    with torch.no_grad():\n",
    "        return model.encode_text(clip.tokenize([text]).to(device))\n",
    "\n",
    "\n",
    "def get_images_from_folder(folder_path):\n",
    "    images = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is an image file\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "            # Open the image using PIL\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            # Append the image to the list\n",
    "            images.append(get_image_embedding(image))\n",
    "\n",
    "    return images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def find_image_embedding_arithmetics(pair1,pair2):\n",
    "    first_pair = pair1.split(':')\n",
    "    second_pair = pair2.split(':')\n",
    "\n",
    "    first_image_embeddings = get_images_from_folder(os.path.join(\"Dataset\" ,first_pair[0]))\n",
    "    second_image_embeddings = get_images_from_folder(os.path.join(\"Dataset\" ,first_pair[1]))\n",
    "    third_image_embeddings = get_images_from_folder(os.path.join(\"Dataset\" ,second_pair[0]))\n",
    "    fourth_image_embeddings = get_images_from_folder(os.path.join(\"Dataset\" ,second_pair[1]))\n",
    "\n",
    "\n",
    "    first_image_embeddings = torch.squeeze(torch.stack(first_image_embeddings, dim=0), dim=1)\n",
    "    second_image_embeddings = torch.squeeze(torch.stack(second_image_embeddings, dim=0), dim=1)\n",
    "    third_image_embeddings = torch.squeeze(torch.stack(third_image_embeddings, dim=0), dim=1)\n",
    "    fourth_image_embeddings = torch.squeeze(torch.stack(fourth_image_embeddings, dim=0), dim=1)\n",
    "\n",
    "    # Average of embeddings\n",
    "    first_image_embeddings = torch.mean(first_image_embeddings, dim=0)\n",
    "    second_image_embeddings = torch.mean(second_image_embeddings, dim=0)\n",
    "    third_image_embeddings = torch.mean(third_image_embeddings, dim=0)\n",
    "    fourth_image_embeddings = torch.mean(fourth_image_embeddings, dim=0)\n",
    "\n",
    "\n",
    "    offset_vector_embedding = second_image_embeddings - first_image_embeddings + third_image_embeddings\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth_image_embeddings, dim=0)\n",
    "\n",
    "    cos_image_before_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "\n",
    "    print(\"Angle for Image embedding before normalizing: Averaging \", cos_image_before_normalization)\n",
    "\n",
    "    rho_image_before_normalization, p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth_image_embeddings.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for Image embedding before normalizing:\", rho_image_before_normalization)\n",
    "\n",
    "\n",
    "    first_image_embeddings = F.normalize(first_image_embeddings , p=2,dim=0)\n",
    "    second_image_embeddings = F.normalize(second_image_embeddings , p=2,dim=0)\n",
    "    third_image_embeddings = F.normalize(third_image_embeddings, p=2,dim=0)\n",
    "    fourth_image_embeddings = F.normalize(fourth_image_embeddings , p=2,dim=0)\n",
    "\n",
    "    offset_vector_embedding = second_image_embeddings - first_image_embeddings + third_image_embeddings\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth_image_embeddings, dim=0)\n",
    "\n",
    "    cos_image_after_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "\n",
    "    print(\"Angle for Image embedding after normalizing: Averaging \", cos_image_after_normalization)\n",
    "\n",
    "    rho_image_after_normalization, p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth_image_embeddings.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for Image embedding after normalizing:\", rho_image_after_normalization)\n",
    "\n",
    "    return cos_image_before_normalization, rho_image_before_normalization,cos_image_after_normalization, rho_image_after_normalization, offset_vector_embedding, fourth_image_embeddings\n",
    "\n",
    "\n",
    "def find_text_embedding_arithmetics(pair1,pair2):\n",
    "    first_pair = pair1.split(':')\n",
    "    second_pair = pair2.split(':')\n",
    "\n",
    "    first = get_text_embedding(first_pair[0]).squeeze(0)\n",
    "    second = get_text_embedding(first_pair[1]).squeeze(0)\n",
    "    third = get_text_embedding(second_pair[0]).squeeze(0)\n",
    "    fourth = get_text_embedding(second_pair[1]).squeeze(0)\n",
    "\n",
    "    offset_vector_embedding = second - first + third\n",
    "\n",
    "    # King - Queen + Man = Woman\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth, dim=0)\n",
    "\n",
    "    cos_text_before_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "    \n",
    "\n",
    "    print(\"Angle for text embedding before normalizing: Averaging \", cos_text_before_normalization)\n",
    "    rho_text_before_normalization , p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for text embedding before normalizing:\", rho_text_before_normalization)\n",
    "\n",
    "\n",
    "    first = F.normalize(first , p=2,dim=0)\n",
    "    second = F.normalize(second , p=2,dim=0)\n",
    "    third = F.normalize(third , p=2,dim=0)\n",
    "    fourth = F.normalize(fourth, p=2,dim=0)\n",
    "\n",
    "    offset_vector_embedding = second - first + third\n",
    "\n",
    "    cos_sim = F.cosine_similarity(offset_vector_embedding, fourth, dim=0)\n",
    "\n",
    "    cos_text_after_normalization = torch.rad2deg(torch.acos(cos_sim))\n",
    "\n",
    "    print(\"Angle for text embedding after normalizing: Averaging \", cos_text_after_normalization)\n",
    "    rho_text_after_normalization, p_value = spearmanr(offset_vector_embedding.cpu().numpy(), fourth.cpu().numpy())\n",
    "    print(\"Spearman's correlation coefficient for text embedding after normalizing:\", rho_text_after_normalization)\n",
    "\n",
    "    return cos_text_before_normalization, rho_text_before_normalization, cos_text_after_normalization, rho_text_after_normalization, offset_vector_embedding, fourth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_file(pair):\n",
    "\n",
    "    pairs = pair.split(\"::\")\n",
    "    pair1 = pairs[0]\n",
    "    pair2 = pairs[1]\n",
    "\n",
    "\n",
    "    cos_image_before_normalization, rho_image_before_normalization,cos_image_after_normalization, rho_image_after_normalization, offset_vector_embedding_image, fourth_image_embedding = find_image_embedding_arithmetics(pair1,pair2)\n",
    "    cos_text_before_normalization, rho_text_before_normalization, cos_text_after_normalization, rho_text_after_normalization, offset_vector_embedding_text, fourth_text_embedding = find_text_embedding_arithmetics(pair1,pair2)\n",
    "\n",
    "    new_data = {\n",
    "        'pair1':pair1,\n",
    "        'pair2':pair2,\n",
    "        'cos_image_before_normalization':cos_image_before_normalization.item(),\n",
    "        'rho_image_before_normalization':rho_image_before_normalization.item(),\n",
    "        'cos_image_after_normalization':cos_image_after_normalization.item(),\n",
    "        'rho_image_after_normalization':rho_image_after_normalization.item(),\n",
    "        'cos_text_before_normalization':cos_text_before_normalization.item(),\n",
    "        'rho_text_before_normalization':rho_text_before_normalization.item(),\n",
    "        'cos_text_after_normalization':cos_text_after_normalization.item(),\n",
    "        'rho_text_after_normalization':rho_text_after_normalization.item()\n",
    "    }\n",
    "\n",
    "    embedding_data = {\n",
    "        'pair1':pair1,\n",
    "        'pair2':pair2,\n",
    "        'offset_vector_embedding_image' : offset_vector_embedding_image,\n",
    "        'fourth_image_embedding' : fourth_image_embedding,\n",
    "        'offset_vector_embedding_text' : offset_vector_embedding_text,\n",
    "        'fourth_text_embedding':fourth_text_embedding\n",
    "    }\n",
    "\n",
    "    return new_data, embedding_data\n",
    "\n",
    "\n",
    "# def process_folder(folder_path,results,embedding_results):\n",
    "#     for file_path in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "#         similarity_data, embedding_data = process_file(file_path, folder_path)\n",
    "#         results.append(similarity_data)\n",
    "#         embedding_results.append(embedding_data)\n",
    "        \n",
    "        \n",
    "\n",
    "#     for sub_folder in os.listdir(folder_path):\n",
    "#         sub_folder_path = os.path.join(folder_path, sub_folder)\n",
    "#         if os.path.isdir(sub_folder_path):\n",
    "#             process_folder(sub_folder_path,results,embedding_results)\n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle for Image embedding before normalizing: Averaging  tensor(52.8115)\n",
      "Spearman's correlation coefficient for Image embedding before normalizing: 0.2508716318478845\n",
      "Angle for Image embedding after normalizing: Averaging  tensor(47.5542)\n",
      "Spearman's correlation coefficient for Image embedding after normalizing: 0.29382182983142785\n",
      "Angle for text embedding before normalizing: Averaging  tensor(43.1167)\n",
      "Spearman's correlation coefficient for text embedding before normalizing: 0.2857623868461107\n",
      "Angle for text embedding after normalizing: Averaging  tensor(38.9310)\n",
      "Spearman's correlation coefficient for text embedding after normalizing: 0.30123405331250497\n",
      "Angle for Image embedding before normalizing: Averaging  tensor(39.0998)\n",
      "Spearman's correlation coefficient for Image embedding before normalizing: 0.3189648636431261\n",
      "Angle for Image embedding after normalizing: Averaging  tensor(38.9435)\n",
      "Spearman's correlation coefficient for Image embedding after normalizing: 0.32035747187603714\n",
      "Angle for text embedding before normalizing: Averaging  tensor(32.9413)\n",
      "Spearman's correlation coefficient for text embedding before normalizing: 0.3415595209484899\n",
      "Angle for text embedding after normalizing: Averaging  tensor(32.3152)\n",
      "Spearman's correlation coefficient for text embedding after normalizing: 0.349616549936485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         lines\u001b[38;5;241m.\u001b[39mappend(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m---> 20\u001b[0m     similarity_data, embedding_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(similarity_data)\n\u001b[0;32m     22\u001b[0m     embedding_results\u001b[38;5;241m.\u001b[39mappend(embedding_data)\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(pair)\u001b[0m\n\u001b[0;32m      9\u001b[0m pair1 \u001b[38;5;241m=\u001b[39m pairs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m pair2 \u001b[38;5;241m=\u001b[39m pairs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m cos_image_before_normalization, rho_image_before_normalization,cos_image_after_normalization, rho_image_after_normalization, offset_vector_embedding_image, fourth_image_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mfind_image_embedding_arithmetics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpair2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m cos_text_before_normalization, rho_text_before_normalization, cos_text_after_normalization, rho_text_after_normalization, offset_vector_embedding_text, fourth_text_embedding \u001b[38;5;241m=\u001b[39m find_text_embedding_arithmetics(pair1,pair2)\n\u001b[0;32m     16\u001b[0m new_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair1\u001b[39m\u001b[38;5;124m'\u001b[39m:pair1,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair2\u001b[39m\u001b[38;5;124m'\u001b[39m:pair2,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrho_text_after_normalization\u001b[39m\u001b[38;5;124m'\u001b[39m:rho_text_after_normalization\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m }\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mfind_image_embedding_arithmetics\u001b[1;34m(pair1, pair2)\u001b[0m\n\u001b[0;32m      8\u001b[0m second_image_embeddings \u001b[38;5;241m=\u001b[39m get_images_from_folder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m ,first_pair[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m      9\u001b[0m third_image_embeddings \u001b[38;5;241m=\u001b[39m get_images_from_folder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m ,second_pair[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m---> 10\u001b[0m fourth_image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_images_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43msecond_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m first_image_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(torch\u001b[38;5;241m.\u001b[39mstack(first_image_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m second_image_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(torch\u001b[38;5;241m.\u001b[39mstack(second_image_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mget_images_from_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m         image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# Append the image to the list\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_image_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m, in \u001b[0;36mget_image_embedding\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_features\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\clip\\model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\clip\\model.py:232\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre(x)\n\u001b[0;32m    231\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[0;32m    235\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_post(x[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\clip\\model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\clip\\model.py:191\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    190\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[1;32m--> 191\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\MS\\COMP545\\project\\Investigating the Arithmetic of Visual Embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to the main 'data' folder\n",
    "# data_folder = 'SemEval-2012-Gold-Ratings'\n",
    "\n",
    "results = []\n",
    "embedding_results = []\n",
    "\n",
    "# Process Training and Testing folders\n",
    "# for sub_folder in ['Training', 'Testing']:\n",
    "#     sub_folder_path = os.path.join(data_folder, sub_folder)\n",
    "#     process_folder(sub_folder_path,results,embedding_results)\n",
    "\n",
    "lines = []\n",
    "\n",
    "with open('pairs.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character at the end of each line and append to the list\n",
    "        lines.append(line.strip())\n",
    "        \n",
    "for pair in lines:\n",
    "    similarity_data, embedding_data = process_file(pair)\n",
    "    results.append(similarity_data)\n",
    "    embedding_results.append(embedding_data)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv('data.csv', index=False)\n",
    "\n",
    "df_embedding = pd.DataFrame(embedding_results)\n",
    "\n",
    "df_embedding.to_csv('embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
